import streamlit as st
import os
from docx import Document
import zipfile
import requests
import pandas as pd 
import json
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
# plt.rcParams['font.family'] = 'Malgun Gothic'
# plt.rcParams['axes.unicode_minus'] = False

# í”„ë¡œì íŠ¸ ë‚´ í°íŠ¸ ê²½ë¡œ <- ì›¹ì•±ì—ì„œ í°íŠ¸ ë³„ë„ ì—†ê¸° ë•Œë¬¸ì— matplotlibì— í°íŠ¸ ë“±ë¡
font_path = os.path.join("font", "NanumGothic-Bold.ttf")
fontprop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = fontprop.get_name()
plt.rcParams['axes.unicode_minus'] = False
fm.fontManager.addfont(font_path)  # ê°•ì œ ë“±ë¡


# import openai
from openai import AzureOpenAI
from dotenv import load_dotenv

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient


######### openai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½ í•¨. #########

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ì‹œ, 
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")
# openai.azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
# openai.api_type = os.getenv("AZURE_OPENAI_API_TYPE")
# openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")

AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_TYPE = os.getenv("AZURE_OPENAI_API_TYPE")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_MODEL")

# print(AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_TYPE)

EMBEDDING_MODEL = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")

# Azure Cognitive Search ì„¤ì •
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX_NAME = os.getenv("SEARCH_INDEX_NAME")

# Azure Cognitive Search í´ë¼ì´ì–¸íŠ¸ ìƒì„±
search_client = SearchClient(
    endpoint=SEARCH_ENDPOINT,
    index_name=SEARCH_INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY)
)

# ì—°ê²° ë¬¸ìì—´ (Azure Portal > Access keys > Connection string)
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
AZURE_BLOB_CONTAINER = os.getenv("AZURE_BLOB_CONTAINER")

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = AzureOpenAI(
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
)


DATA_PATH = "streamlit_data/ia_docx_parsed.json"
WORD_DOCS_DIR = "streamlit_data/ia_docx"


# --- ê³µí†µ í•¨ìˆ˜ ---
def load_data():
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# íŒ€ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def extract_teams(data):
    teams = set()
    for doc in data:
        for part in doc.get("parts", []):
            teams.add(part["part_name"])
    return sorted(list(teams))

def get_next_doc_id(data):
    return f"{len(data) + 1:03d}"

# --- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ---
def get_embedding(text):
    response = client.embeddings.create(
        input=text,
        model=EMBEDDING_MODEL
    )
    # st.write("Embedding response:", response.data[0].embedding)
    return response.data[0].embedding

def search_similar_docs(embedding):
    headers = {
        "Content-Type": "application/json",
        "api-key": SEARCH_API_KEY
    }
    payload = {
        "vector": embedding,
        "top": 5,
        "fields": "text_vector",
        "select": "chunk,title",
        "vectorFields": "text_vector"
    }
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-preview"
    # url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2025-01-01-preview"
    response = requests.post(url, headers=headers, json=payload)
    return response.json().get("value", [])


# --- GPTë¥¼ í†µí•œ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ---
def analyze_parts(prompt, similar_docs):
    examples = "\n".join([
        f"{i+1}. ìš”êµ¬ì‚¬í•­: \"{doc['chunk'][:80]}...\"\n   ì—°ê´€ íŒŒíŠ¸: í™”ë©´ê°œë°œ, ëª¨ë‹ˆí„°ë§"
        for i, doc in enumerate(similar_docs)
    ])

    # GPTì—ê²Œ ë³´ë‚¼ ì‹œìŠ¤í…œ ë° ì‚¬ìš©ì ë©”ì‹œì§€
    system_msg = "ë„ˆëŠ” IT íšŒì‚¬ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ìš´ì˜íŒ€ì—ì„œ ì‚¬ìš©í•˜ëŠ” AI ë¶„ì„ ë„ìš°ë¯¸ì•¼.\n" \
                 "ê³ ê°ì´ ì…ë ¥í•œ ìš”êµ¬ì‚¬í•­ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ íŒŒíŠ¸(ì—…ë¬´ ì˜ì—­)ì™€ ì—°ê´€ ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•´. " \
                 "ê° íŒŒíŠ¸ë³„ ì—°ê´€ë„ë¥¼ 0.0 ~ 1.0 ì‚¬ì´ ìˆ˜ì¹˜ë¡œ ì¶œë ¥í•´ì¤˜." \
                 "í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ IAë¬¸ì„œë„ ë³´ì—¬ì¤˜"

    user_msg = f"""
[ì‹œìŠ¤í…œ ì—­í•  ì§€ì‹œ]ë„ˆëŠ” IT íšŒì‚¬ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ìš´ì˜íŒ€ì—ì„œ ì‚¬ìš©í•˜ëŠ” AI ë¶„ì„ ë„ìš°ë¯¸ì•¼.
ë„ˆì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ì•„:
1. ê³ ê°ì´ ì…ë ¥í•œ ìš”êµ¬ì‚¬í•­ê³¼ ìœ ì‚¬í•œ ì‚°ì¶œë¬¼(IA ë¬¸ì„œ)ì„ ë³´ì—¬ì£¼ê³ ,
2. ê·¸ ë¬¸ì„œë“¤ê³¼ ë¹„êµí•´ë´¤ì„ ë•Œ ì–´ë–¤ íŒŒíŠ¸(ì—…ë¬´ ì˜ì—­)ê°€ ì´ ìš”êµ¬ì‚¬í•­ê³¼ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì¶”ë¡ í•´,
3. ê° íŒŒíŠ¸ë³„ ì—°ê´€ë„ë¥¼ ìˆ˜ì¹˜(0~1.0)ë¡œ ì¶œë ¥í•´ì¤˜. (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì—°ê´€ì´ ë†’ìŒ)

[ì…ë ¥ëœ ê³ ê° ìš”êµ¬ì‚¬í•­]
ìš”êµ¬ì‚¬í•­: "{prompt}" â† ì‚¬ìš©ìì˜ ì‹¤ì œ ì…ë ¥ì´ ë“¤ì–´ê°ˆ ìë¦¬

[IA ë¬¸ì„œ ì˜ˆì‹œ]
â€» ë‹¤ìŒì€ ë²¡í„° ìœ ì‚¬ë„ë¡œ ê²€ìƒ‰ëœ ì‹¤ì œ IA ì‚°ì¶œë¬¼ë“¤ì´ë‹¤. ê° ë¬¸ì„œì—ëŠ” ê´€ë ¨ëœ ìš”êµ¬ì‚¬í•­ê³¼ ë‹´ë‹¹ íŒŒíŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.
{examples}

[ë¶„ì„ ë°©ì‹]
- ê° IA ë¬¸ì„œì˜ ë‚´ìš©, ì—°ê´€ íŒŒíŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ìš”êµ¬ì‚¬í•­ì´ ì–´ë–¤ íŒŒíŠ¸ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•´.
- ê¸°ëŠ¥ ì˜ì—­, UI/UX, ëª¨ë‹ˆí„°ë§, ìš´ì˜ ìë™í™”, ìˆ˜ë‚©, ìš”ê¸ˆ ë“±ì˜ í‚¤ì›Œë“œë¥¼ í™œìš©í•´ì„œ íŒë‹¨í•˜ë˜, ë‹¨ìˆœ í‚¤ì›Œë“œ ì¼ì¹˜ë³´ë‹¤ ë¬¸ë§¥ì˜ ëª©ì ê³¼ ì‘ì—… íë¦„ì„ ì¤‘ì ìœ¼ë¡œ íŒë‹¨í•´.
- ê´€ë ¨ëœ íŒŒíŠ¸ëŠ” ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë©°, ê° íŒŒíŠ¸ì— ëŒ€í•´ 0.0 ~ 1.0 ì‚¬ì´ ì—°ê´€ë„ ì ìˆ˜ë¥¼ ì •ìˆ˜ ë‘ ìë¦¿ìˆ˜ ì†Œìˆ˜ë¡œ í‘œì‹œí•´ì¤˜. ì—°ê´€ë„ê°€ 0ì´ì–´ë„ ë¬´ì¡°ê±´ ê²°ê³¼ì— ë„£ì–´ì¤˜
- ë°˜ë“œì‹œ ì‹¤ì œ ì‚°ì¶œë¬¼ ë¬¸ì„œ ë‚´ìš©ê³¼ ë¹„êµí•˜ë©° íŒë‹¨í•´.

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "ìš”êµ¬ì‚¬í•­": "",
  "ì—°ê´€íŒŒíŠ¸": [
    {{"íŒŒíŠ¸": "Part A", "ì—°ê´€ë„": 0.85}},
    {{"íŒŒíŠ¸": "Part C", "ì—°ê´€ë„": 0.6}}
  ],
  "ìš”ì•½": "",
  "ë¶„ì„ ì´ìœ " : ""
}}

[ì œí•œ ì‚¬í•­]
- ë°˜ë“œì‹œ ì •í•´ì§„ íŒŒíŠ¸ ëª©ë¡ ë‚´ì—ì„œë§Œ íŒë‹¨í•´ì¤˜: ["ìš”ê¸ˆì •ë³´", "ìˆ˜ë‚©", "í™”ë©´ê°œë°œ", "ëª¨ë‹ˆí„°ë§", "ìš´ì˜"]
- ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë¬¸ë§¥ì´ ì• ë§¤í•  ê²½ìš°, í•´ë‹¹ íŒŒíŠ¸ëŠ” ìƒëµí•´ë„ ë¼.

"""

    # RAG íŒ¨í„´ì„ ì ìš©í•˜ê¸° ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì •
    rag_params = {
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": SEARCH_ENDPOINT,
                    "index_name": SEARCH_INDEX_NAME,
                    "authentication": {
                        "type": "api_key",
                        "key": SEARCH_API_KEY
                    },
                    "query_type": "vector",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": EMBEDDING_MODEL,
                    }
                }
            }
        ]
    }

    # st.write("Full prompt for GPT:", full_prompt)

    response = client.chat.completions.create(
        # engine="gpt-4",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ],
        model = DEPLOYMENT_NAME, # ë°°í¬ì´ë¦„
        extra_body=rag_params
    )

    #st.write("GPT response:", response.choices[0].message.content)
    # print(response.choices[0].message.content)

    try:
        return eval(response.choices[0].message.content)
    except:
        return {}



# --- Streamlit UI ---
st.set_page_config(page_title="íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="centered")

st.sidebar.title("ğŸ“‚ ê¸°ëŠ¥ ì„ íƒ")
mode = st.sidebar.radio("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ìš”êµ¬ì‚¬í•­ ë¶„ì„", "IA ë¬¸ì„œê¸°ë°˜ ì¡°íšŒ"])

st.sidebar.markdown("### ğŸ“¥ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ")

# âœ… streamlit_data/ia_docx ë‚´ ë¬¸ì„œ ì••ì¶• ë‹¤ìš´ë¡œë“œ
if os.listdir(WORD_DOCS_DIR):
    zip_path = "ia_word_documents_all.zip"
    with zipfile.ZipFile(zip_path, "w") as zipf:
        for fname in os.listdir(WORD_DOCS_DIR):
            fpath = os.path.join(WORD_DOCS_DIR, fname)
            zipf.write(fpath, arcname=fname)

    with open(zip_path, "rb") as f:
        st.sidebar.download_button(
            label="ğŸ“¦ ì „ì²´ ë¬¸ì„œ ì••ì¶• ë‹¤ìš´ë¡œë“œ",
            data=f,
            file_name="ia_word_documents_all.zip",
            mime="application/zip"
        )

st.sidebar.markdown("---")

# --- IA ë¬¸ì„œ ì—…ë¡œë“œ ëª¨ë“œ ---
if mode == "IA ë¬¸ì„œê¸°ë°˜ ì¡°íšŒ":
    st.title("ğŸ“„ IA ë¬¸ì„œ ì—…ë¡œë“œ ë° ì €ì¥")

    # ğŸ“¥ JSON ë¡œë“œ
    @st.cache_data
    def load_json():
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    data = load_data()

    # --- ë¬¸ì„œ ì—…ë¡œë“œ ---
    uploaded_file = st.file_uploader("IA Word ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["docx"])
    if uploaded_file:
        # ì €ì¥
        filename = uploaded_file.name
        save_path = os.path.join(WORD_DOCS_DIR, filename)
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # íŒŒì‹±
        docx = Document(save_path)
        paragraphs = [p.text.strip() for p in docx.paragraphs if p.text.strip()]
        content = paragraphs

        # íŒŒì¼ëª…ì—ì„œ req_id, team ì¶”ì¶œ
        parts = filename.replace(".docx", "").split("_")
        req_id = parts[0] if len(parts) > 0 else f"REQ{len(data)+1:03d}"
        team = parts[1] if len(parts) > 1 else "Unknown"

        # ìƒˆ í•­ëª© ìƒì„±
        new_doc = {
            "req_id": req_id,
            "team": team,
            "filename": filename,
            "content": content
        }

        # ê¸°ì¡´ JSONì— ì¶”ê°€
        data.append(new_doc)
        with open(DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        st.success(f"âœ… ì—…ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ: {filename}")


    st.markdown("----")

    # --- í•„í„°ë§

    # ğŸ§© íŒ€ ëª©ë¡ ìƒì„± (ì˜ˆì™¸ ë°©ì§€)
    teams = sorted(set(d.get("team", "Unknown") for d in data if d.get("team")))
    selected_team = st.selectbox("ğŸ‘¨â€ğŸ’» íŒ€ ì„ íƒ (ì—°ê´€ ë¬¸ì„œ í•„í„°ë§)", ["ì „ì²´"] + teams)

    # ğŸ” í‚¤ì›Œë“œ ì…ë ¥
    keyword = st.text_input("ğŸ” í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ë‚´ìš© í•„í„°ë§ (ì„ íƒ)", placeholder="ì˜ˆ: ì˜¤ë¥˜, LAN, ë¡œê·¸ ê¸°ë¡ ë“±")

    # ğŸ” í•„í„°ë§
    filtered = []
    for d in data:
        team = d.get("team", "")
        if selected_team != "ì „ì²´" and team != selected_team:
            continue
        if keyword:
            content = d.get("content", [])
            if not any(keyword in para for para in content):
                continue
        filtered.append(d)

    # ğŸ“„ ê³¼ì œ ì„ íƒ
    if filtered:
        def safe_title(doc):
            return f"{doc.get('req_id', 'REQXXX')} - {doc.get('filename', 'unknown.docx')}"
        
        req_ids = [safe_title(d) for d in filtered]
        selected_doc = st.selectbox("ğŸ“„ ê³¼ì œ ì„ íƒ", req_ids)
        selected_id = selected_doc.split(" - ")[0]

        doc = next(
            (d for d in filtered if d.get("req_id") == selected_id and safe_title(d) == selected_doc),
            None
        )

        # ğŸ“‘ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
        if doc:
            st.subheader(f"ğŸ“Œ ê³¼ì œ ID: {doc.get('req_id', 'REQXXX')}")
            st.markdown(f"**ì—°ê´€ íŒ€:** `{doc.get('team', 'Unknown')}`")
            st.markdown(f"**íŒŒì¼ëª…:** `{doc.get('filename', '-')}`")
            st.divider()
            st.subheader("ğŸ“‘ ë¬¸ì„œ ë‚´ìš©")

            for i, para in enumerate(doc.get("content", []), 1):
                if keyword and keyword in para:
                    st.markdown(f"âœ… {para}")
                else:
                    st.markdown(f"{para}")
        else:
            st.warning("ğŸ“„ ì„ íƒëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ğŸ” ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")





# --- ìš”êµ¬ì‚¬í•­ ë¶„ì„ ëª¨ë“œ ---
elif mode == "ìš”êµ¬ì‚¬í•­ ë¶„ì„":
    st.title("ğŸ“Š ê³ ê° ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„")
    prompt = st.text_area("ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”", height=150)

    if st.button("ë¶„ì„ ì‹œì‘") and prompt:
        with st.spinner("ë²¡í„°í™” ë° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
            embedding = get_embedding(prompt)
            similar_docs = search_similar_docs(embedding)
            # st.write("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:", similar_docs) --ì ê¹ ì œê±°

        with st.spinner("GPTë¡œ ì—°ê´€ë„ ë¶„ì„ ì¤‘..."):
            result = analyze_parts(prompt, similar_docs)

        if result:
            st.success("ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")

            print(result)


            # max_part = max(result, key=result.get)
            # for part, score in result.items():
            #     if part == max_part:
            #         st.markdown(f"<span style='color:red; font-weight:bold'>{part}: {score:.2f}</span>", unsafe_allow_html=True)
            #     else:
            #         st.write(f"{part}: {score:.2f}")
                    
            # df = pd.DataFrame(list(result.items()), columns=["íŒŒíŠ¸", "ì—°ê´€ë„ ì ìˆ˜"])
            # st.bar_chart(df.set_index("íŒŒíŠ¸"))

            
            # ê²°ê³¼ íŒŒì‹±
            try :

                # ë¬¸ìì—´ì´ë©´ json.loads, dictë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if isinstance(result, str):
                    try:
                        result = json.loads(result)
                    except json.JSONDecodeError as e:
                        st.error(f"âŒ GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        result = None
                elif isinstance(result, dict):
                    result = result
                else:
                    st.error("âŒ GPT ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
                    result = None

            
                # ğŸ“ ìš”ì•½ ë° ë¶„ì„ í‘œì‹œ
                st.markdown("### ğŸ“ ìš”êµ¬ì‚¬í•­")
                st.write(result.get("ìš”êµ¬ì‚¬í•­", "-"))

                st.markdown("### ğŸ“Œ ìš”ì•½")
                st.success(result.get("ìš”ì•½", "-"))

                st.markdown("### ğŸ§  ë¶„ì„ ì´ìœ ")
                st.info(result.get("ë¶„ì„ ì´ìœ ", "-"))

                # ğŸ“‘ ìœ ì‚¬ ë¬¸ì„œ
                if "ìœ ì‚¬ IA ë¬¸ì„œ" in result:
                    st.markdown("### ğŸ“„ ìœ ì‚¬ IA ë¬¸ì„œ")
                    st.code(result["ìœ ì‚¬ IA ë¬¸ì„œ"])

                # ğŸ“Š ì—°ê´€ë„ ì‹œê°í™”
                st.markdown("### ğŸ“Š íŒŒíŠ¸ë³„ ì—°ê´€ë„")

                df = pd.DataFrame(result["ì—°ê´€íŒŒíŠ¸"])
                df = df.sort_values("ì—°ê´€ë„", ascending=True)

                fig, ax = plt.subplots(figsize=(6, 3.5))
                ax.barh(df["íŒŒíŠ¸"], df["ì—°ê´€ë„"])
                ax.set_xlim(0, 1.0)
                ax.set_xlabel("ì—°ê´€ë„ (0.0 ~ 1.0)")
                ax.set_title("íŒŒíŠ¸ë³„ ì—°ê´€ë„ ë¶„ì„ ê²°ê³¼")
                st.pyplot(fig)


            except Exception as e:
                st.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.error("ì§€ê¸ˆì€ GPTê°€ í‡´ê·¼í–ˆì–´ìš”. ë‹¤ìŒì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”!")


