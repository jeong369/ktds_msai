import streamlit as st
import os
from docx import Document
import zipfile
import requests
import pandas as pd 
import json

# import openai
from openai import AzureOpenAI
from dotenv import load_dotenv

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient


######### openai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½ í•¨. #########

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ì‹œ, 
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")
# openai.azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
# openai.api_type = os.getenv("AZURE_OPENAI_API_TYPE")
# openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")

AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_TYPE = os.getenv("AZURE_OPENAI_API_TYPE")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_MODEL")

# print(AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_TYPE)

EMBEDDING_MODEL = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")

# Azure Cognitive Search ì„¤ì •
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX_NAME = os.getenv("SEARCH_INDEX_NAME")

# Azure Cognitive Search í´ë¼ì´ì–¸íŠ¸ ìƒì„±
search_client = SearchClient(
    endpoint=SEARCH_ENDPOINT,
    index_name=SEARCH_INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY)
)

# ì—°ê²° ë¬¸ìì—´ (Azure Portal > Access keys > Connection string)
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
AZURE_BLOB_CONTAINER = os.getenv("AZURE_BLOB_CONTAINER")

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = AzureOpenAI(
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
)


DATA_PATH = "streamlit_data/ia_docx_parsed.json"
WORD_DOCS_DIR = "streamlit_data/ia_docx"


# --- ê³µí†µ í•¨ìˆ˜ ---
def load_data():
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# íŒ€ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def extract_teams(data):
    teams = set()
    for doc in data:
        for part in doc.get("parts", []):
            teams.add(part["part_name"])
    return sorted(list(teams))

def get_next_doc_id(data):
    return f"{len(data) + 1:03d}"

# --- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ---
def get_embedding(text):
    response = client.embeddings.create(
        input=text,
        model=EMBEDDING_MODEL
    )
    # st.write("Embedding response:", response.data[0].embedding)
    return response.data[0].embedding

def search_similar_docs(embedding):
    headers = {
        "Content-Type": "application/json",
        "api-key": SEARCH_API_KEY
    }
    payload = {
        "vector": embedding,
        "top": 5,
        "fields": "text_vector",
        "select": "chunk,title",
        "vectorFields": "text_vector"
    }
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-preview"
    # url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2025-01-01-preview"
    response = requests.post(url, headers=headers, json=payload)
    return response.json().get("value", [])


# --- GPTë¥¼ í†µí•œ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ---
def analyze_parts(prompt, similar_docs):
    examples = "\n".join([
        f"{i+1}. ìš”êµ¬ì‚¬í•­: \"{doc['chunk'][:80]}...\"\n   ì—°ê´€ íŒŒíŠ¸: í™”ë©´ê°œë°œ, ëª¨ë‹ˆí„°ë§"
        for i, doc in enumerate(similar_docs)
    ])

    # GPTì—ê²Œ ë³´ë‚¼ ì‹œìŠ¤í…œ ë° ì‚¬ìš©ì ë©”ì‹œì§€
    system_msg = "ë„ˆëŠ” IT íšŒì‚¬ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ìš´ì˜íŒ€ì—ì„œ ì‚¬ìš©í•˜ëŠ” AI ë¶„ì„ ë„ìš°ë¯¸ì•¼.\n" \
                 "ê³ ê°ì´ ì…ë ¥í•œ ìš”êµ¬ì‚¬í•­ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ íŒŒíŠ¸(ì—…ë¬´ ì˜ì—­)ì™€ ì—°ê´€ ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•´. " \
                 "ê° íŒŒíŠ¸ë³„ ì—°ê´€ë„ë¥¼ 0.0 ~ 1.0 ì‚¬ì´ ìˆ˜ì¹˜ë¡œ ì¶œë ¥í•´ì¤˜." \
                 "í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ IAë¬¸ì„œë„ ë³´ì—¬ì¤˜"

    user_msg = f"""
ë„ˆëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œíŒ€ì˜ AI ë„ìš°ë¯¸ì•¼. ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì„œ ì•„ë˜ íŒŒíŠ¸ë“¤ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ ìˆëŠ”ì§€ 0~1 ì‚¬ì´ ì ìˆ˜ë¡œ í‘œí˜„í•´ì¤˜.

ìš”êµ¬ì‚¬í•­: "{prompt}"

ì˜ˆì‹œ:
{examples}

ì‘ë‹µ í˜•ì‹(JSON):
{{
  "ìš”ê¸ˆì±…ì •": 0.0,
  "ìˆ˜ë‚©": 0.0,
  "í™”ë©´ê°œë°œ": 0.0,
  "ëª¨ë‹ˆí„°ë§": 0.0,
  "ì¥ì• ëŒ€ì‘": 0.0
}}


"""

    # RAG íŒ¨í„´ì„ ì ìš©í•˜ê¸° ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì •
    rag_params = {
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": SEARCH_ENDPOINT,
                    "index_name": SEARCH_INDEX_NAME,
                    "authentication": {
                        "type": "api_key",
                        "key": SEARCH_API_KEY
                    },
                    "query_type": "vector",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": EMBEDDING_MODEL,
                    }
                }
            }
        ]
    }

    # st.write("Full prompt for GPT:", full_prompt)

    response = client.chat.completions.create(
        # engine="gpt-4",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ],
        model = DEPLOYMENT_NAME, # ë°°í¬ì´ë¦„
        extra_body=rag_params
    )

    # st.write("GPT response:", response.choices[0])
    # print(response.choices[0].message.content)

    try:
        return eval(response.choices[0].message.content)
    except:
        return {}



# --- Streamlit UI ---
st.set_page_config(page_title="íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="centered")

st.sidebar.title("ğŸ“‚ ê¸°ëŠ¥ ì„ íƒ")
mode = st.sidebar.radio("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", ["IA ë¬¸ì„œê¸°ë°˜ ì¡°íšŒ", "ìš”êµ¬ì‚¬í•­ ë¶„ì„"])

# --- IA ë¬¸ì„œ ì—…ë¡œë“œ ëª¨ë“œ ---
if mode == "IA ë¬¸ì„œê¸°ë°˜ ì¡°íšŒ":
    st.title("ğŸ“„ IA ë¬¸ì„œ ì—…ë¡œë“œ ë° ì €ì¥")
    

    # ğŸ“¥ JSON ë¡œë“œ
    @st.cache_data
    def load_json():
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    data = load_data()

    # ğŸ§© íŒ€ ëª©ë¡ ìƒì„± (ì˜ˆì™¸ ë°©ì§€)
    teams = sorted(set(d.get("team", "Unknown") for d in data if d.get("team")))
    selected_team = st.selectbox("ğŸ‘¨â€ğŸ’» íŒ€ ì„ íƒ (ì—°ê´€ ë¬¸ì„œ í•„í„°ë§)", ["ì „ì²´"] + teams)

    # ğŸ” í‚¤ì›Œë“œ ì…ë ¥
    keyword = st.text_input("ğŸ” í‚¤ì›Œë“œë¡œ ë¬¸ì„œ ë‚´ìš© í•„í„°ë§ (ì„ íƒ)", placeholder="ì˜ˆ: ì˜¤ë¥˜, LAN, ë¡œê·¸ ê¸°ë¡ ë“±")

    # ğŸ” í•„í„°ë§
    filtered = []
    for d in data:
        team = d.get("team", "")
        if selected_team != "ì „ì²´" and team != selected_team:
            continue
        if keyword:
            content = d.get("content", [])
            if not any(keyword in para for para in content):
                continue
        filtered.append(d)

    # ğŸ“„ ê³¼ì œ ì„ íƒ
    if filtered:
        def safe_title(doc):
            return f"{doc.get('req_id', 'REQXXX')} - {doc.get('filename', 'unknown.docx')}"
        
        req_ids = [safe_title(d) for d in filtered]
        selected_doc = st.selectbox("ğŸ“„ ê³¼ì œ ì„ íƒ", req_ids)
        selected_id = selected_doc.split(" - ")[0]

        doc = next(
            (d for d in filtered if d.get("req_id") == selected_id and safe_title(d) == selected_doc),
            None
        )

        # ğŸ“‘ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
        if doc:
            st.subheader(f"ğŸ“Œ ê³¼ì œ ID: {doc.get('req_id', 'REQXXX')}")
            st.markdown(f"**ì—°ê´€ íŒ€:** `{doc.get('team', 'Unknown')}`")
            st.markdown(f"**íŒŒì¼ëª…:** `{doc.get('filename', '-')}`")
            st.divider()
            st.subheader("ğŸ“‘ ë¬¸ì„œ ë‚´ìš©")

            for i, para in enumerate(doc.get("content", []), 1):
                if keyword and keyword in para:
                    st.markdown(f"âœ… {para}")
                else:
                    st.markdown(f"{para}")
        else:
            st.warning("ğŸ“„ ì„ íƒëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ğŸ” ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")



    
    # --- ë¬¸ì„œ ì—…ë¡œë“œ (êµì²´í•  ì½”ë“œ ì‹œì‘) ---
    uploaded_file = st.file_uploader("IA Word ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["docx"])
    if uploaded_file:
        # ì €ì¥
        filename = uploaded_file.name
        save_path = os.path.join(WORD_DOCS_DIR, filename)
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # íŒŒì‹±
        docx = Document(save_path)
        paragraphs = [p.text.strip() for p in docx.paragraphs if p.text.strip()]
        content = paragraphs

        # íŒŒì¼ëª…ì—ì„œ req_id, team ì¶”ì¶œ
        parts = filename.replace(".docx", "").split("_")
        req_id = parts[0] if len(parts) > 0 else f"REQ{len(data)+1:03d}"
        team = parts[1] if len(parts) > 1 else "Unknown"

        # ìƒˆ í•­ëª© ìƒì„±
        new_doc = {
            "req_id": req_id,
            "team": team,
            "filename": filename,
            "content": content
        }

        # ê¸°ì¡´ JSONì— ì¶”ê°€
        data.append(new_doc)
        with open(DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        st.success(f"âœ… ì—…ë¡œë“œ ë° ì €ì¥ ì™„ë£Œ: {filename}")


    # ì „ì²´ ì••ì¶• ë‹¤ìš´ë¡œë“œ
    if os.listdir(WORD_DOCS_DIR):
        with zipfile.ZipFile("ia_word_documents_all.zip", "w") as zipf:
            for fname in os.listdir(WORD_DOCS_DIR):
                fpath = os.path.join(WORD_DOCS_DIR, fname)
                zipf.write(fpath, arcname=fname)
        with open("ia_word_documents_all.zip", "rb") as f:
            st.download_button("ğŸ“¦ ì „ì²´ ë¬¸ì„œ ì••ì¶• ë‹¤ìš´ë¡œë“œ", f, file_name="ia_word_documents_all.zip")




# --- ìš”êµ¬ì‚¬í•­ ë¶„ì„ ëª¨ë“œ ---
elif mode == "ìš”êµ¬ì‚¬í•­ ë¶„ì„":
    st.title("ğŸ“Š ê³ ê° ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„")
    prompt = st.text_area("ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”", height=150)

    if st.button("ë¶„ì„ ì‹œì‘") and prompt:
        with st.spinner("ë²¡í„°í™” ë° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
            embedding = get_embedding(prompt)
            similar_docs = search_similar_docs(embedding)
            # st.write("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:", similar_docs) --ì ê¹ ì œê±°

        with st.spinner("GPTë¡œ ì—°ê´€ë„ ë¶„ì„ ì¤‘..."):
            result = analyze_parts(prompt, similar_docs)

        if result:
            st.success("ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")

            # print(result)
            max_part = max(result, key=result.get)
            for part, score in result.items():
                if part == max_part:
                    st.markdown(f"<span style='color:red; font-weight:bold'>{part}: {score:.2f}</span>", unsafe_allow_html=True)
                else:
                    st.write(f"{part}: {score:.2f}")
                    
            df = pd.DataFrame(list(result.items()), columns=["íŒŒíŠ¸", "ì—°ê´€ë„ ì ìˆ˜"])
            st.bar_chart(df.set_index("íŒŒíŠ¸"))

            
            
            ## ê²°ê³¼ íŒŒì‹±
            # try :
            #     print("ë¶„ì„ ê²°ê³¼:", result)

            #     # ğŸ¯ JSON ë¶€ë¶„ ì¶”ì¶œ
            #     json_match = re.search(r'\{.*?\}', result, re.DOTALL)
            #     json_part = json.loads(json_match.group()) if json_match else {}

            #     # ğŸ“œ ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ
            #     text_part = result[json_match.end():].strip() if json_match else result.strip()

            #     st.write(json_match)
            #     st.write(json_part)
            #     st.write(text_part)

            #     # âœ… ì—°ê´€ë„ ë§‰ëŒ€ ê·¸ë˜í”„
            #     st.subheader("ğŸ”¢ íŒŒíŠ¸ ì—°ê´€ë„ ì‹œê°í™”")
            #     df = pd.DataFrame(json_part.items(), columns=["íŒŒíŠ¸", "ì—°ê´€ë„"]).sort_values(by="ì—°ê´€ë„", ascending=True)

            #     fig, ax = plt.subplots()
            #     bars = ax.barh(df["íŒŒíŠ¸"], df["ì—°ê´€ë„"])
            #     for bar in bars:
            #         if bar.get_width() == max(df["ì—°ê´€ë„"]):
            #             bar.set_color("red")
            #     ax.set_xlabel("ì—°ê´€ë„ (0 ~ 1.0)")
            #     ax.set_title("ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ íŒŒíŠ¸ë³„ ì—°ê´€ë„")
            #     st.pyplot(fig)

            #     # ğŸ“ GPT ì„¤ëª… í…ìŠ¤íŠ¸ ì¶œë ¥
            #     st.subheader("ğŸ—’ï¸ GPT ë¶„ì„ ì„¤ëª…")
            #     st.markdown(text_part)


            # except Exception as e:
            #     st.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.error("GPT ì‘ë‹µì„ ë¶„ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")


