import streamlit as st
import json
import os
from docx import Document
import zipfile
import requests
import pandas as pd 

# import openai
from openai import AzureOpenAI
from dotenv import load_dotenv

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient


######### openai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½ í•¨. #########

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ì‹œ, 
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")
# openai.azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
# openai.api_type = os.getenv("AZURE_OPENAI_API_TYPE")
# openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")

AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_TYPE = os.getenv("AZURE_OPENAI_API_TYPE")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_MODEL")

# print(AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_TYPE)

EMBEDDING_MODEL = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")

# Azure Cognitive Search ì„¤ì •
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX_NAME = os.getenv("SEARCH_INDEX_NAME")

# Azure Cognitive Search í´ë¼ì´ì–¸íŠ¸ ìƒì„±
search_client = SearchClient(
    endpoint=SEARCH_ENDPOINT,
    index_name=SEARCH_INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY)
)

# ì—°ê²° ë¬¸ìì—´ (Azure Portal > Access keys > Connection string)
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
AZURE_BLOB_CONTAINER = os.getenv("AZURE_BLOB_CONTAINER")

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = AzureOpenAI(
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
)



# --- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ---
def get_embedding(text):
    response = client.embeddings.create(
        input=text,
        model=EMBEDDING_MODEL
    )
    # st.write("Embedding response:", response.data[0].embedding)
    return response.data[0].embedding

def search_similar_docs(embedding):
    headers = {
        "Content-Type": "application/json",
        "api-key": SEARCH_API_KEY
    }
    payload = {
        "vector": embedding,
        "top": 5,
        "fields": "text_vector",
        "select": "chunk,title",
        "vectorFields": "text_vector"
    }
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-preview"
    # url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2025-01-01-preview"
    response = requests.post(url, headers=headers, json=payload)
    return response.json().get("value", [])

# --- GPTë¥¼ í†µí•œ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ---
def analyze_parts(prompt, similar_docs):
    examples = "\n".join([
        f"ìš”êµ¬ì‚¬í•­: {doc['chunk'][:150]}...\nì—°ê´€ íŒŒíŠ¸: í™”ë©´ê°œë°œ, ëª¨ë‹ˆí„°ë§"
        for doc in similar_docs
    ])

    full_prompt = f"""
ë„ˆëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œíŒ€ì˜ AI ë„ìš°ë¯¸ì•¼. ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì„œ ì•„ë˜ íŒŒíŠ¸ë“¤ê³¼ ì–¼ë§ˆë‚˜ ì—°ê´€ ìˆëŠ”ì§€ 0~1 ì‚¬ì´ ì ìˆ˜ë¡œ í‘œí˜„í•´ì¤˜.

ìš”êµ¬ì‚¬í•­: "{prompt}"

ì˜ˆì‹œ:
{examples}

ì‘ë‹µ í˜•ì‹(JSON):
{{
  "ìš”ê¸ˆì±…ì •": 0.0,
  "ìˆ˜ë‚©": 0.0,
  "í™”ë©´ê°œë°œ": 0.0,
  "ëª¨ë‹ˆí„°ë§": 0.0,
  "ì¥ì• ëŒ€ì‘": 0.0
}}
"""

    # RAG íŒ¨í„´ì„ ì ìš©í•˜ê¸° ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì •
    rag_params = {
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": SEARCH_ENDPOINT,
                    "index_name": SEARCH_INDEX_NAME,
                    "authentication": {
                        "type": "api_key",
                        "key": SEARCH_API_KEY
                    },
                    "query_type": "vector",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": EMBEDDING_MODEL,
                    }
                }
            }
        ]
    }

    # st.write("Full prompt for GPT:", full_prompt)

    response = client.chat.completions.create(
        # engine="gpt-4",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” íŒŒíŠ¸ë³„ ì—°ê´€ë„ ë¶„ì„ ì „ë¬¸ê°€ì•¼."},
            {"role": "user", "content": full_prompt}
        ],
        model = DEPLOYMENT_NAME, # ë°°í¬ì´ë¦„
        extra_body=rag_params
    )

    st.write("GPT response:", response.choices[0].message.content)
    try:
        return eval(response.choices[0].message.content)
    except:
        return {}

# --- Streamlit UI ---
st.set_page_config(page_title="íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="centered")
st.title("ğŸ“Š ê³ ê° ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„")

prompt = st.text_area("ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”", height=150)

if st.button("ë¶„ì„ ì‹œì‘") and prompt:
    with st.spinner("ë²¡í„°í™” ë° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
        embedding = get_embedding(prompt)
        similar_docs = search_similar_docs(embedding)
        st.write("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:", similar_docs)

    with st.spinner("GPTë¡œ ì—°ê´€ë„ ë¶„ì„ ì¤‘..."):
        result = analyze_parts(prompt, similar_docs)

    if result:
        st.success("ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")

        max_part = max(result, key=result.get)
        for part, score in result.items():
            if part == max_part:
                st.markdown(f"<span style='color:red; font-weight:bold'>{part}: {score:.2f}</span>", unsafe_allow_html=True)
            else:
                st.write(f"{part}: {score:.2f}")
                
        df = pd.DataFrame(list(result.items()), columns=["íŒŒíŠ¸", "ì—°ê´€ë„ ì ìˆ˜"])
        st.bar_chart(df.set_index("íŒŒíŠ¸"))
    else:
        st.error("GPT ì‘ë‹µì„ ë¶„ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")




# # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# conn_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
# container_name = os.getenv("AZURE_BLOB_CONTAINER")

# # Azure BlobServiceClient ìƒì„±
# blob_service_client = BlobServiceClient.from_connection_string(conn_str)
# container_client = blob_service_client.get_container_client(container_name)

# # Streamlit UI
# st.markdown('----')
# uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.docx)", type=["docx"])

# if uploaded_file is not None:
#     try:
#         # íŒŒì¼ ì—…ë¡œë“œ â†’ Blob Storage
#         blob_client = container_client.get_blob_client(uploaded_file.name)
#         blob_client.upload_blob(uploaded_file.getvalue(), overwrite=True)

#         st.success(f"âœ… {uploaded_file.name} íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
#     except Exception as e:
#         st.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")