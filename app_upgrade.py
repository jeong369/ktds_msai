import streamlit as st
import json
import re
import os
from docx import Document
import zipfile
import requests
import pandas as pd 
import matplotlib.pyplot as plt

# import openai
from openai import AzureOpenAI
from dotenv import load_dotenv

from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.storage.blob import BlobServiceClient


######### openai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½ í•¨. #########

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv(override=True)

# í™˜ê²½ë³€ìˆ˜ ë¡œë”© ì‹œ, 
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")
# openai.azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
# openai.api_type = os.getenv("AZURE_OPENAI_API_TYPE")
# openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")

AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_TYPE = os.getenv("AZURE_OPENAI_API_TYPE")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_CHAT_MODEL")

# print(AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_TYPE)

EMBEDDING_MODEL = os.getenv("AZURE_OPENAI_EMBEDDING_MODEL")

# Azure Cognitive Search ì„¤ì •
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX_NAME = os.getenv("SEARCH_INDEX_NAME")

# Azure Cognitive Search í´ë¼ì´ì–¸íŠ¸ ìƒì„±
search_client = SearchClient(
    endpoint=SEARCH_ENDPOINT,
    index_name=SEARCH_INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY)
)

# ì—°ê²° ë¬¸ìì—´ (Azure Portal > Access keys > Connection string)
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
AZURE_BLOB_CONTAINER = os.getenv("AZURE_BLOB_CONTAINER")

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = AzureOpenAI(
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
)



# --- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ---
def get_embedding(text):
    response = client.embeddings.create(
        input=text,
        model=EMBEDDING_MODEL
    )
    # st.write("Embedding response:", response.data[0].embedding)
    return response.data[0].embedding

def search_similar_docs(embedding):
    headers = {
        "Content-Type": "application/json",
        "api-key": SEARCH_API_KEY
    }
    payload = {
        "vector": embedding,
        "top": 5,
        "fields": "text_vector",
        "select": "chunk,title",
        "vectorFields": "text_vector"
    }
    url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-preview"
    # url = f"{SEARCH_ENDPOINT}/indexes/{SEARCH_INDEX_NAME}/docs/search?api-version=2025-01-01-preview"
    response = requests.post(url, headers=headers, json=payload)
    return response.json().get("value", [])


# --- GPTë¥¼ í†µí•œ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ---
def analyze_parts(prompt, similar_docs):
    examples = "\n".join([
        f"{i+1}. ìš”êµ¬ì‚¬í•­: \"{doc['chunk'][:80]}...\"\n   ì—°ê´€ íŒŒíŠ¸: í™”ë©´ê°œë°œ, ëª¨ë‹ˆí„°ë§"
        for i, doc in enumerate(similar_docs)
    ])

    # GPTì—ê²Œ ë³´ë‚¼ ì‹œìŠ¤í…œ ë° ì‚¬ìš©ì ë©”ì‹œì§€
    system_msg = "ë„ˆëŠ” IT íšŒì‚¬ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ìš´ì˜íŒ€ì—ì„œ ì‚¬ìš©í•˜ëŠ” AI ë¶„ì„ ë„ìš°ë¯¸ì•¼.\n" \
                 "ê³ ê°ì´ ì…ë ¥í•œ ìš”êµ¬ì‚¬í•­ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ íŒŒíŠ¸(ì—…ë¬´ ì˜ì—­)ì™€ ì—°ê´€ ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•´. " \
                 "ê° íŒŒíŠ¸ë³„ ì—°ê´€ë„ë¥¼ 0.0 ~ 1.0 ì‚¬ì´ ìˆ˜ì¹˜ë¡œ ì¶œë ¥í•´ì¤˜."

    user_msg = f"""
[ì…ë ¥ëœ ê³ ê° ìš”êµ¬ì‚¬í•­]
ìš”êµ¬ì‚¬í•­: "{prompt}"

[IA ë¬¸ì„œ ì˜ˆì‹œ]
{examples}

[ë¶„ì„ ë°©ì‹]
- ê° IA ë¬¸ì„œì˜ ë‚´ìš©, ì—°ê´€ íŒŒíŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ìš”êµ¬ì‚¬í•­ì´ ì–´ë–¤ íŒŒíŠ¸ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•´.
- ê¸°ëŠ¥ ì˜ì—­, UI/UX, ëª¨ë‹ˆí„°ë§, ìš´ì˜ ìë™í™”, ìˆ˜ë‚©, ìš”ê¸ˆ ë“±ì˜ í‚¤ì›Œë“œë¥¼ í™œìš©í•´ì„œ íŒë‹¨í•˜ë˜, ë‹¨ìˆœ í‚¤ì›Œë“œ ì¼ì¹˜ë³´ë‹¤ ë¬¸ë§¥ì˜ ëª©ì ê³¼ ì‘ì—… íë¦„ì„ ì¤‘ì ìœ¼ë¡œ íŒë‹¨í•´.
- ê´€ë ¨ëœ íŒŒíŠ¸ëŠ” ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë©°, ê° íŒŒíŠ¸ì— ëŒ€í•´ 0.0 ~ 1.0 ì‚¬ì´ ì—°ê´€ë„ ì ìˆ˜ë¥¼ ì†Œìˆ˜ì  ë‘ ìë¦¿ìˆ˜ë¡œ í‘œí˜„í•´ì¤˜.
- ë°˜ë“œì‹œ ë‹¤ìŒ íŒŒíŠ¸ ëª©ë¡ ë‚´ì—ì„œ íŒë‹¨í•  ê²ƒ: ["ìš”ê¸ˆì •ë³´", "ìˆ˜ë‚©", "í™”ë©´ê°œë°œ", "ëª¨ë‹ˆí„°ë§", "ìš´ì˜"]

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
{{
  "ìš”ê¸ˆì •ë³´": 0.42,
  "ìˆ˜ë‚©": 0.15,
  "í™”ë©´ê°œë°œ": 0.65,
  "ëª¨ë‹ˆí„°ë§": 0.91,
  "ìš´ì˜": 0.30
}}

[ì œí•œ ì‚¬í•­]
- ë°˜ë“œì‹œ ì •í•´ì§„ íŒŒíŠ¸ ëª©ë¡ ë‚´ì—ì„œë§Œ íŒë‹¨í•´ì¤˜: ["ìš”ê¸ˆì •ë³´", "ìˆ˜ë‚©", "í™”ë©´ê°œë°œ", "ëª¨ë‹ˆí„°ë§", "ìš´ì˜"]
- ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë¬¸ë§¥ì´ ì• ë§¤í•  ê²½ìš°, í•´ë‹¹ íŒŒíŠ¸ëŠ” ìƒëµí•´ë„ ë¼.
    """

    # RAG íŒ¨í„´ì„ ì ìš©í•˜ê¸° ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì •
    rag_params = {
        "data_sources": [
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": SEARCH_ENDPOINT,
                    "index_name": SEARCH_INDEX_NAME,
                    "authentication": {
                        "type": "api_key",
                        "key": SEARCH_API_KEY
                    },
                    "query_type": "vector",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": EMBEDDING_MODEL,
                    }
                }
            }
        ]
    }

    # st.write("Full prompt for GPT:", full_prompt)

    response = client.chat.completions.create(
        # engine="gpt-4",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ],
        model = DEPLOYMENT_NAME, # ë°°í¬ì´ë¦„
        extra_body=rag_params
    )

    # st.write("GPT response:", response.choices[0])
    # print(response.choices[0].message.content)

    try:
        return eval(response.choices[0].message.content)
    except:
        return {}

# --- Streamlit UI ---
st.set_page_config(page_title="íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="centered")
st.title("ğŸ“Š ê³ ê° ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒŒíŠ¸ ì—°ê´€ë„ ë¶„ì„")

prompt = st.text_area("ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”", height=150)

if st.button("ë¶„ì„ ì‹œì‘") and prompt:
    with st.spinner("ë²¡í„°í™” ë° ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
        embedding = get_embedding(prompt)
        similar_docs = search_similar_docs(embedding)
        st.write("ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:", similar_docs) #ì ê¹ ì œê±°

    with st.spinner("GPTë¡œ ì—°ê´€ë„ ë¶„ì„ ì¤‘..."):
        result = analyze_parts(prompt, similar_docs)

    if result:
        st.success("ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")

        # print(result)
        # max_part = max(result, key=result.get)
        # for part, score in result.items():
        #     if part == max_part:
        #         st.markdown(f"<span style='color:red; font-weight:bold'>{part}: {score:.2f}</span>", unsafe_allow_html=True)
        #     else:
        #         st.write(f"{part}: {score:.2f}")
                
        # df = pd.DataFrame(list(result.items()), columns=["íŒŒíŠ¸", "ì—°ê´€ë„ ì ìˆ˜"])
        # st.bar_chart(df.set_index("íŒŒíŠ¸"))

        
        
        # ê²°ê³¼ íŒŒì‹±
        try :
            print("ë¶„ì„ ê²°ê³¼:", result)

            # ğŸ¯ JSON ë¶€ë¶„ ì¶”ì¶œ
            json_match = re.search(r'\{.*?\}', result, re.DOTALL)
            json_part = json.loads(json_match.group()) if json_match else {}

            # ğŸ“œ ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_part = result[json_match.end():].strip() if json_match else result.strip()

            st.write(json_match)
            st.write(json_part)
            st.write(text_part)

            # âœ… ì—°ê´€ë„ ë§‰ëŒ€ ê·¸ë˜í”„
            st.subheader("ğŸ”¢ íŒŒíŠ¸ ì—°ê´€ë„ ì‹œê°í™”")
            df = pd.DataFrame(json_part.items(), columns=["íŒŒíŠ¸", "ì—°ê´€ë„"]).sort_values(by="ì—°ê´€ë„", ascending=True)

            fig, ax = plt.subplots()
            bars = ax.barh(df["íŒŒíŠ¸"], df["ì—°ê´€ë„"])
            for bar in bars:
                if bar.get_width() == max(df["ì—°ê´€ë„"]):
                    bar.set_color("red")
            ax.set_xlabel("ì—°ê´€ë„ (0 ~ 1.0)")
            ax.set_title("ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ íŒŒíŠ¸ë³„ ì—°ê´€ë„")
            st.pyplot(fig)

            # ğŸ“ GPT ì„¤ëª… í…ìŠ¤íŠ¸ ì¶œë ¥
            st.subheader("ğŸ—’ï¸ GPT ë¶„ì„ ì„¤ëª…")
            st.markdown(text_part)


        except Exception as e:
            st.error(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    else:
        st.error("GPT ì‘ë‹µì„ ë¶„ì„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")




# # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# conn_str = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
# container_name = os.getenv("AZURE_BLOB_CONTAINER")

# # Azure BlobServiceClient ìƒì„±
# blob_service_client = BlobServiceClient.from_connection_string(conn_str)
# container_client = blob_service_client.get_container_client(container_name)

# # Streamlit UI
# st.markdown('----')
# uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.docx)", type=["docx"])

# if uploaded_file is not None:
#     try:
#         # íŒŒì¼ ì—…ë¡œë“œ â†’ Blob Storage
#         blob_client = container_client.get_blob_client(uploaded_file.name)
#         blob_client.upload_blob(uploaded_file.getvalue(), overwrite=True)

#         st.success(f"âœ… {uploaded_file.name} íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
#     except Exception as e:
#         st.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")